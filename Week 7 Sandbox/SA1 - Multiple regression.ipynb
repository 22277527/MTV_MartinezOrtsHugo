{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SA1 - Multiple regression**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Objective:**  \n",
    "To assess and strengthen understanding of multiple regression concepts, including coefficient interpretation, model validation, multicollinearity, and residual diagnostics.  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1: Interpretation of Coefficients**  \n",
    "A researcher conducts a study to predict apartment prices in a city based on three variables:  \n",
    "\n",
    "- **X₁** = Size in square meters  \n",
    "- **X₂** = Number of rooms  \n",
    "- **X₃** = Location (1 if in a central area, 0 otherwise)  \n",
    "\n",
    "The estimated model is:  \n",
    "$$\n",
    "Price = 50000 + 2000X₁ + 15000X₂ + 30000X₃\n",
    "$$\n",
    "\n",
    "1. What is the estimated price for an apartment of 80 m², with 3 rooms, located in a central area?  \n",
    "2. What is the interpretation of the coefficient for variable $X_3$?  \n",
    "3. What happens if $X_3$ takes the value of 0?  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated price is:\n",
      "285000\n"
     ]
    }
   ],
   "source": [
    "estimated_price = 50000 + 2000*80 + 15000*3 + 30000*1\n",
    "print(\"The estimated price is:\")\n",
    "print(estimated_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "\n",
    "The coefficient of \\(X_3\\) (30,000) indicates that, on average, an apartment located in a central area will have its price increase by 30,000 monetary units compared to an apartment that is not in a central area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "\n",
    "If the X3 takes the value of 0 the apartment price will be 30000 less compared to an apartment located in a central area,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2: Multicollinearity**  \n",
    "A dataset contains information on student exam performance with the following variables:  \n",
    "\n",
    "- **Y** = Exam score  \n",
    "- **X₁** = Study hours  \n",
    "- **X₂** = Sleep hours  \n",
    "- **X₃** = Socioeconomic level  \n",
    "\n",
    "The researcher finds that the correlation between $X_1$ and $X_2$ is -0.95.  \n",
    "\n",
    "1. Why can high correlation between $X_1$ and $X_2$ be a problem in multiple regression?  \n",
    "2. What technique could be used to reduce the impact of multicollinearity?  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why can high correlation between \n",
    "𝑋\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    "  and \n",
    "𝑋\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    "  be a problem in multiple regression?\n",
    "A high correlation between two predictor variables, such as \n",
    "𝑋\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    "  (study hours) and \n",
    "𝑋\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    "  (sleep hours), can cause multicollinearity in multiple regression. Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, making it difficult to separate their individual effects on the dependent variable.\n",
    "\n",
    "Specifically, a correlation of -0.95 between \n",
    "𝑋\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    "  and \n",
    "𝑋\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    "  suggests that they are almost perfectly inversely related. This high correlation can cause the following problems:\n",
    "\n",
    "Unstable coefficients: The estimated coefficients for \n",
    "𝑋\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    "  and \n",
    "𝑋\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    "  may be very sensitive to small changes in the data. This can lead to large standard errors for the coefficients and unreliable estimates.\n",
    "Difficulty in interpretation: When two variables are highly correlated, it becomes challenging to interpret the individual impact of each variable on the dependent variable (exam score, \n",
    "𝑌\n",
    "Y) because their effects are confounded.\n",
    "Overfitting: The model might overfit the data, capturing noise rather than true relationships, and this can reduce the generalizability of the model to new data.\n",
    "\n",
    "\n",
    "2. What technique could be used to reduce the impact of multicollinearity?\n",
    "To reduce the impact of multicollinearity, several techniques can be applied:\n",
    "\n",
    "Remove one of the correlated variables: If \n",
    "𝑋\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    "  and \n",
    "𝑋\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    "  are highly correlated, removing one of them from the model can help resolve the issue.\n",
    "Principal Component Analysis (PCA): PCA is a technique that transforms the correlated variables into a smaller set of uncorrelated variables (principal components). These new components can then be used in the regression model.\n",
    "Ridge regression or Lasso regression: These are regularization techniques that add a penalty to the model to reduce the size of the coefficients, thus mitigating the impact of multicollinearity. Ridge regression adds an L2 penalty, while Lasso adds an L1 penalty, which can also perform variable selection.\n",
    "By using these techniques, you can address multicollinearity and improve the stability and interpretability of your regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Por qué puede ser un problema la alta correlación entre \n",
    "𝑋\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    "  y \n",
    "𝑋\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    "  en una regresión múltiple?\n",
    "Una alta correlación entre dos variables predictoras, como \n",
    "𝑋\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    "  (horas de estudio) y \n",
    "𝑋\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    "  (horas de sueño), puede causar multicolinealidad en una regresión múltiple. La multicolinealidad ocurre cuando dos o más variables independientes en un modelo de regresión están altamente correlacionadas, lo que dificulta separar sus efectos individuales sobre la variable dependiente.\n",
    "\n",
    "Específicamente, una correlación de -0.95 entre \n",
    "𝑋\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    "  y \n",
    "𝑋\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    "  sugiere que están casi perfectamente inversamente relacionadas. Esta alta correlación puede causar los siguientes problemas:\n",
    "\n",
    "Coeficientes inestables: Los coeficientes estimados para \n",
    "𝑋\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    "  y \n",
    "𝑋\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    "  pueden ser muy sensibles a pequeños cambios en los datos. Esto puede generar errores estándar grandes para los coeficientes y estimaciones poco confiables.\n",
    "Dificultad en la interpretación: Cuando dos variables están altamente correlacionadas, se hace difícil interpretar el impacto individual de cada una sobre la variable dependiente (puntaje del examen, \n",
    "𝑌\n",
    "Y), ya que sus efectos están confundidos.\n",
    "Sobreajuste (overfitting): El modelo puede sobreajustarse a los datos, capturando ruido en lugar de relaciones reales, lo que reduce la capacidad de generalización del modelo a nuevos datos.\n",
    "\n",
    "\n",
    "2. ¿Qué técnica se podría usar para reducir el impacto de la multicolinealidad?\n",
    "Para reducir el impacto de la multicolinealidad, se pueden aplicar varias técnicas:\n",
    "\n",
    "Eliminar una de las variables correlacionadas: Si \n",
    "𝑋\n",
    "1\n",
    "X \n",
    "1\n",
    "​\n",
    "  y \n",
    "𝑋\n",
    "2\n",
    "X \n",
    "2\n",
    "​\n",
    "  están altamente correlacionadas, eliminar una de ellas del modelo puede ayudar a resolver el problema.\n",
    "Análisis de Componentes Principales (PCA): El PCA es una técnica que transforma las variables correlacionadas en un conjunto más pequeño de variables no correlacionadas (componentes principales). Estos nuevos componentes se pueden usar en el modelo de regresión.\n",
    "Regresión Ridge o Lasso: Son técnicas de regularización que agregan una penalización al modelo para reducir el tamaño de los coeficientes, mitigando así el impacto de la multicolinealidad. La regresión Ridge agrega una penalización L2, mientras que Lasso agrega una penalización L1, que también puede realizar selección de variables.\n",
    "Al utilizar estas técnicas, se puede abordar la multicolinealidad y mejorar la estabilidad e interpretabilidad del modelo de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3: Model Fit Assessment**  \n",
    "A financial analyst builds a multiple regression model to predict investment performance. The results include:  \n",
    "\n",
    "- **Adjusted $ R^2 $ = 0.78**  \n",
    "- **Model p-value < 0.01**  \n",
    "- **Coefficient for a key variable = 0.001, with p-value = 0.45**  \n",
    "\n",
    "1. Is the model statistically significant overall? Why?  \n",
    "2. How would you interpret the coefficient of the key variable?  \n",
    "3. What would you do if the key variable is not significant but should theoretically be included in the model?  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 4: Residual Diagnostics**  \n",
    "A researcher wants to check whether the residuals of their model meet the assumptions of normality and homoscedasticity. The following plots are generated:  \n",
    "\n",
    "- **Histogram of residuals**: Shows a right-skewed distribution.  \n",
    "- **Residuals vs. fitted values plot**: Displays a fan-shaped pattern.  \n",
    "\n",
    "1. What does the skewness in the residuals histogram indicate?  \n",
    "2. How would you interpret the fan-shaped pattern in the residuals plot?  \n",
    "3. What transformations could you apply to the model to correct these issues?  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 5: Variable Selection**  \n",
    "A team of economists is developing a model to predict monthly household electricity consumption. Initially, they include the following variables:  \n",
    "\n",
    "- **X₁** = Number of people in the household  \n",
    "- **X₂** = House size in square meters  \n",
    "- **X₃** = Household income level  \n",
    "- **X₄** = Age of the head of the household  \n",
    "\n",
    "After training the model, they obtain the following results:  \n",
    "\n",
    "| Variable | Coefficient | p-value |\n",
    "|----------|------------|---------|\n",
    "| X₁       | 25         | 0.01    |\n",
    "| X₂       | 10         | 0.03    |\n",
    "| X₃       | 0.5        | 0.48    |\n",
    "| X₄       | -3         | 0.65    |\n",
    "\n",
    "1. Which of these variables would you remove from the model? Justify your answer.  \n",
    "2. What method could you use to automatically select the best variables for the model?  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise bulletin will help reinforce your understanding of multiple regression. Once completed, review your answers based on theory, and if possible, test some of these concepts using Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
