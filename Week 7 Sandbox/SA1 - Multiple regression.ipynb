{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SA1 - Multiple regression**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Objective:**  \n",
    "To assess and strengthen understanding of multiple regression concepts, including coefficient interpretation, model validation, multicollinearity, and residual diagnostics.  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1: Interpretation of Coefficients**  \n",
    "A researcher conducts a study to predict apartment prices in a city based on three variables:  \n",
    "\n",
    "- **Xâ‚** = Size in square meters  \n",
    "- **Xâ‚‚** = Number of rooms  \n",
    "- **Xâ‚ƒ** = Location (1 if in a central area, 0 otherwise)  \n",
    "\n",
    "The estimated model is:  \n",
    "$$\n",
    "Price = 50000 + 2000Xâ‚ + 15000Xâ‚‚ + 30000Xâ‚ƒ\n",
    "$$\n",
    "\n",
    "1. What is the estimated price for an apartment of 80 mÂ², with 3 rooms, located in a central area?  \n",
    "2. What is the interpretation of the coefficient for variable $X_3$?  \n",
    "3. What happens if $X_3$ takes the value of 0?  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated price is:\n",
      "285000\n"
     ]
    }
   ],
   "source": [
    "estimated_price = 50000 + 2000*80 + 15000*3 + 30000*1\n",
    "print(\"The estimated price is:\")\n",
    "print(estimated_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "\n",
    "The coefficient of \\(X_3\\) (30,000) indicates that, on average, an apartment located in a central area will have its price increase by 30,000 monetary units compared to an apartment that is not in a central area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "\n",
    "If the X3 takes the value of 0 the apartment price will be 30000 less compared to an apartment located in a central area,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2: Multicollinearity**  \n",
    "A dataset contains information on student exam performance with the following variables:  \n",
    "\n",
    "- **Y** = Exam score  \n",
    "- **Xâ‚** = Study hours  \n",
    "- **Xâ‚‚** = Sleep hours  \n",
    "- **Xâ‚ƒ** = Socioeconomic level  \n",
    "\n",
    "The researcher finds that the correlation between $X_1$ and $X_2$ is -0.95.  \n",
    "\n",
    "1. Why can high correlation between $X_1$ and $X_2$ be a problem in multiple regression?  \n",
    "2. What technique could be used to reduce the impact of multicollinearity?  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why can high correlation between \n",
    "ğ‘‹\n",
    "1\n",
    "X \n",
    "1\n",
    "â€‹\n",
    "  and \n",
    "ğ‘‹\n",
    "2\n",
    "X \n",
    "2\n",
    "â€‹\n",
    "  be a problem in multiple regression?\n",
    "A high correlation between two predictor variables, such as \n",
    "ğ‘‹\n",
    "1\n",
    "X \n",
    "1\n",
    "â€‹\n",
    "  (study hours) and \n",
    "ğ‘‹\n",
    "2\n",
    "X \n",
    "2\n",
    "â€‹\n",
    "  (sleep hours), can cause multicollinearity in multiple regression. Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, making it difficult to separate their individual effects on the dependent variable.\n",
    "\n",
    "Specifically, a correlation of -0.95 between \n",
    "ğ‘‹\n",
    "1\n",
    "X \n",
    "1\n",
    "â€‹\n",
    "  and \n",
    "ğ‘‹\n",
    "2\n",
    "X \n",
    "2\n",
    "â€‹\n",
    "  suggests that they are almost perfectly inversely related. This high correlation can cause the following problems:\n",
    "\n",
    "Unstable coefficients: The estimated coefficients for \n",
    "ğ‘‹\n",
    "1\n",
    "X \n",
    "1\n",
    "â€‹\n",
    "  and \n",
    "ğ‘‹\n",
    "2\n",
    "X \n",
    "2\n",
    "â€‹\n",
    "  may be very sensitive to small changes in the data. This can lead to large standard errors for the coefficients and unreliable estimates.\n",
    "Difficulty in interpretation: When two variables are highly correlated, it becomes challenging to interpret the individual impact of each variable on the dependent variable (exam score, \n",
    "ğ‘Œ\n",
    "Y) because their effects are confounded.\n",
    "Overfitting: The model might overfit the data, capturing noise rather than true relationships, and this can reduce the generalizability of the model to new data.\n",
    "\n",
    "\n",
    "2. What technique could be used to reduce the impact of multicollinearity?\n",
    "To reduce the impact of multicollinearity, several techniques can be applied:\n",
    "\n",
    "Remove one of the correlated variables: If \n",
    "ğ‘‹\n",
    "1\n",
    "X \n",
    "1\n",
    "â€‹\n",
    "  and \n",
    "ğ‘‹\n",
    "2\n",
    "X \n",
    "2\n",
    "â€‹\n",
    "  are highly correlated, removing one of them from the model can help resolve the issue.\n",
    "Principal Component Analysis (PCA): PCA is a technique that transforms the correlated variables into a smaller set of uncorrelated variables (principal components). These new components can then be used in the regression model.\n",
    "Ridge regression or Lasso regression: These are regularization techniques that add a penalty to the model to reduce the size of the coefficients, thus mitigating the impact of multicollinearity. Ridge regression adds an L2 penalty, while Lasso adds an L1 penalty, which can also perform variable selection.\n",
    "By using these techniques, you can address multicollinearity and improve the stability and interpretability of your regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Â¿Por quÃ© puede ser un problema la alta correlaciÃ³n entre \n",
    "ğ‘‹\n",
    "1\n",
    "X \n",
    "1\n",
    "â€‹\n",
    "  y \n",
    "ğ‘‹\n",
    "2\n",
    "X \n",
    "2\n",
    "â€‹\n",
    "  en una regresiÃ³n mÃºltiple?\n",
    "Una alta correlaciÃ³n entre dos variables predictoras, como \n",
    "ğ‘‹\n",
    "1\n",
    "X \n",
    "1\n",
    "â€‹\n",
    "  (horas de estudio) y \n",
    "ğ‘‹\n",
    "2\n",
    "X \n",
    "2\n",
    "â€‹\n",
    "  (horas de sueÃ±o), puede causar multicolinealidad en una regresiÃ³n mÃºltiple. La multicolinealidad ocurre cuando dos o mÃ¡s variables independientes en un modelo de regresiÃ³n estÃ¡n altamente correlacionadas, lo que dificulta separar sus efectos individuales sobre la variable dependiente.\n",
    "\n",
    "EspecÃ­ficamente, una correlaciÃ³n de -0.95 entre \n",
    "ğ‘‹\n",
    "1\n",
    "X \n",
    "1\n",
    "â€‹\n",
    "  y \n",
    "ğ‘‹\n",
    "2\n",
    "X \n",
    "2\n",
    "â€‹\n",
    "  sugiere que estÃ¡n casi perfectamente inversamente relacionadas. Esta alta correlaciÃ³n puede causar los siguientes problemas:\n",
    "\n",
    "Coeficientes inestables: Los coeficientes estimados para \n",
    "ğ‘‹\n",
    "1\n",
    "X \n",
    "1\n",
    "â€‹\n",
    "  y \n",
    "ğ‘‹\n",
    "2\n",
    "X \n",
    "2\n",
    "â€‹\n",
    "  pueden ser muy sensibles a pequeÃ±os cambios en los datos. Esto puede generar errores estÃ¡ndar grandes para los coeficientes y estimaciones poco confiables.\n",
    "Dificultad en la interpretaciÃ³n: Cuando dos variables estÃ¡n altamente correlacionadas, se hace difÃ­cil interpretar el impacto individual de cada una sobre la variable dependiente (puntaje del examen, \n",
    "ğ‘Œ\n",
    "Y), ya que sus efectos estÃ¡n confundidos.\n",
    "Sobreajuste (overfitting): El modelo puede sobreajustarse a los datos, capturando ruido en lugar de relaciones reales, lo que reduce la capacidad de generalizaciÃ³n del modelo a nuevos datos.\n",
    "\n",
    "\n",
    "2. Â¿QuÃ© tÃ©cnica se podrÃ­a usar para reducir el impacto de la multicolinealidad?\n",
    "Para reducir el impacto de la multicolinealidad, se pueden aplicar varias tÃ©cnicas:\n",
    "\n",
    "Eliminar una de las variables correlacionadas: Si \n",
    "ğ‘‹\n",
    "1\n",
    "X \n",
    "1\n",
    "â€‹\n",
    "  y \n",
    "ğ‘‹\n",
    "2\n",
    "X \n",
    "2\n",
    "â€‹\n",
    "  estÃ¡n altamente correlacionadas, eliminar una de ellas del modelo puede ayudar a resolver el problema.\n",
    "AnÃ¡lisis de Componentes Principales (PCA): El PCA es una tÃ©cnica que transforma las variables correlacionadas en un conjunto mÃ¡s pequeÃ±o de variables no correlacionadas (componentes principales). Estos nuevos componentes se pueden usar en el modelo de regresiÃ³n.\n",
    "RegresiÃ³n Ridge o Lasso: Son tÃ©cnicas de regularizaciÃ³n que agregan una penalizaciÃ³n al modelo para reducir el tamaÃ±o de los coeficientes, mitigando asÃ­ el impacto de la multicolinealidad. La regresiÃ³n Ridge agrega una penalizaciÃ³n L2, mientras que Lasso agrega una penalizaciÃ³n L1, que tambiÃ©n puede realizar selecciÃ³n de variables.\n",
    "Al utilizar estas tÃ©cnicas, se puede abordar la multicolinealidad y mejorar la estabilidad e interpretabilidad del modelo de regresiÃ³n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3: Model Fit Assessment**  \n",
    "A financial analyst builds a multiple regression model to predict investment performance. The results include:  \n",
    "\n",
    "- **Adjusted $ R^2 $ = 0.78**  \n",
    "- **Model p-value < 0.01**  \n",
    "- **Coefficient for a key variable = 0.001, with p-value = 0.45**  \n",
    "\n",
    "1. Is the model statistically significant overall? Why?  \n",
    "2. How would you interpret the coefficient of the key variable?  \n",
    "3. What would you do if the key variable is not significant but should theoretically be included in the model?  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 4: Residual Diagnostics**  \n",
    "A researcher wants to check whether the residuals of their model meet the assumptions of normality and homoscedasticity. The following plots are generated:  \n",
    "\n",
    "- **Histogram of residuals**: Shows a right-skewed distribution.  \n",
    "- **Residuals vs. fitted values plot**: Displays a fan-shaped pattern.  \n",
    "\n",
    "1. What does the skewness in the residuals histogram indicate?  \n",
    "2. How would you interpret the fan-shaped pattern in the residuals plot?  \n",
    "3. What transformations could you apply to the model to correct these issues?  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 5: Variable Selection**  \n",
    "A team of economists is developing a model to predict monthly household electricity consumption. Initially, they include the following variables:  \n",
    "\n",
    "- **Xâ‚** = Number of people in the household  \n",
    "- **Xâ‚‚** = House size in square meters  \n",
    "- **Xâ‚ƒ** = Household income level  \n",
    "- **Xâ‚„** = Age of the head of the household  \n",
    "\n",
    "After training the model, they obtain the following results:  \n",
    "\n",
    "| Variable | Coefficient | p-value |\n",
    "|----------|------------|---------|\n",
    "| Xâ‚       | 25         | 0.01    |\n",
    "| Xâ‚‚       | 10         | 0.03    |\n",
    "| Xâ‚ƒ       | 0.5        | 0.48    |\n",
    "| Xâ‚„       | -3         | 0.65    |\n",
    "\n",
    "1. Which of these variables would you remove from the model? Justify your answer.  \n",
    "2. What method could you use to automatically select the best variables for the model?  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise bulletin will help reinforce your understanding of multiple regression. Once completed, review your answers based on theory, and if possible, test some of these concepts using Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
